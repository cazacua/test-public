import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
# Provide the correct file path to the winequality-red.csv file
df = pd.read_csv(r'C:\Users\ctw03293\Downloads\winequality-red.csv')
df.head()
df['quality'].value_counts()
df.columns = df.columns.str.replace(' ','_') #replace empty with underscores
df.head(1) #verifying
df.columns[df.isna().any()]#identoify columns in df with at leats one misisng values
sns.barplot(x='quality',y='citric_acid',data=df) # seaborn to see citric acid against quality
sns.barplot(x='quality',y='chlorides',data=df) # seaborn to see chlorides against quality
sns.barplot(x='quality',y='residual_sugar',data=df) # seaborn to see quality against residual sugar
sns.barplot(x='citric_acid',y='fixed_acidity',data=df)

sns.lineplot(x='citric_acid',y='fixed_acidity',data=df) # the barplot was too dense

#transforming the variable quality in a binary clasisfication , needed for further modelling actions. chosen 6 as
def qualityupdate(df):
    for i,row in df.iterrows():
        val = row['quality']
        if val  <=6:
            df.at[i,'quality']=0
        else:
            df.at[i,'quality']=1
qualityupdate(df)
df.head(1000)
#test train split
from sklearn.model_selection import train_test_split
#separating the feature-independent values from the target var- dependent var , where x will contain the features except quality, that is a dependent var and will be placed in y series
x= df.drop(['quality'],axis=1)#create a new df, x , by droping quality
y=df['quality']#creating a new y series, with the values of the column quality from original df
xtrain,xtest,ytrain,ytest = train_test_split(x,y,test_size=0.2,random_state=42) #using train test split fucntion , 20% of daata will be used for test and 805% for train.  rand state 42 to have the same split each time the code is run
print(y)
#if (y == 1).any():
   # print(y[y == 1]) just wanted to see if there is also 1
#standardisation is needed , as visually it was visible that the variavles were having outliers
from sklearn.preprocessing import StandardScaler
#using the above standardisation for both train and test
sc= StandardScaler()
xtrain = sc.fit_transform(xtrain)
xtest = sc.fit_transform(xtest)
from sklearn.svm import SVC # will be used for classification task
reg = SVC() #assign svc to reg variable , to be tonfigured and trained
reg.fit(xtrain,ytrain)
reg.fit(xtrain,ytrain) #outpur svc()
reg.score(xtest,ytest) # calculation or r squared score of a liner regression model, fitting the test data
#output 0.875
yp = reg.predict(xtest) # store predicted values in yp, of the xtest predictions on the test data, for regression model
from sklearn.metrics import confusion_matrix# to evaluate the performance of a clssification model by ciomparing actual versus predicted
c = confusion_matrix(ytest,yp) 3 computing confusion matrix of predicted values yp and the actual values ytest
c #output array([[268,   5],        [ 35,  12]], dtype=int64)
sns.heatmap(c) # heatmap of confuison matrix to see the no of instances clasisfied correctly or incorectlu
import seaborn as sns
import matplotlib.pyplot as plt
# Assuming c is the confusion matrix
# Display the heatmap
sns.heatmap(c, annot=True, fmt='d', cmap='Blues')
plt.show()
import numpy as np
# Assuming c is the confusion matrix
confusion_matrix_list = c.tolist()
print(confusion_matrix_list) #output  [[TN268,FP 5], [FN35, TP12]]
from sklearn.model_selection import GridSearchCV # will be used to sear a parameter grid to find the best parameteres for our ml model
model = GridSearchCV(reg, {# estimatr or the ml model for which hyperparameter are being tuned
    'C': [0.1, 0.4, 0.8, 1.0, 1.3], # disctionary containing the hyperparameters to be tuned along their possible values
    'gamma': [0.1, 0.4, 0.8, 1.0, 1.3],
    'kernel': ['rbf', 'linear']
}, scoring='accuracy', cv=10)# scoring specifies the no of folds in the cross validation
model
#creating the hiperparameter to turn of the machine lerning model
# output svc
model.fit(xtrain,ytrain) #training the ml, using the pevious model above that can be regression, classificaion or anything else. fit is the model to train the model . during the process th emodle learns the patters in the inut dat and adjust its internarla parameters to make predictions
model.best_params_ #"{'C': 1.3, 'gamma': 1.0, 'kernel': 'rbf'}
#assigning a dictionary to var model c = 1.3 , is a regularisation parameter that controls the tradeoff between achieving a low training error and a low testing error. it is moderate, so it allows balanced  misclasifications .
# gamma defines the influence f a sinlge training examplewith low values mening far. it is high, so the gaussion function has a small variance
#kernel this parameter specifies the kernel type used in my algo. . thr radial basis functionis a popular kernel in svm , for nonlinear classifications
mod = SVC(C=1.3,gamma=1.0,kernel='rbf') #creating an svc named mod
mod.fit(xtrain,ytrain)# train the model using mod, adjusting the parameters to minimise the difference btween predicted and actual target values
mod.score(xtest,ytest) # output 0.896875
# creating a Random Forest Classifier
from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier(n_estimators=200) # initialising random forests classifier at 200 decision trees
rfc.fit(xtrain,ytrain)
rfc.score(xtest,ytest) # output 0.8875
from sklearn.model_selection import cross_val_score
rfc2 = cross_val_score(estimator=rfc,X=xtrain,y=ytrain,cv=10) # cross validation on random forects classifiers, with 10 folds for cross validation
rfc2.mean() # output 0.9124569389763779
#prediction:
df.head(1)
#output     7.4 0.7 0.0 1.9 0.076   11.0    34.0    0.9978  3.51    0.56    9.4 0
a = [[7.4,0.7,0.0,1.9,0.076,11.0,34.0,0.9978,3.51,0.56,9.4]] #initilaising to first output
mod.predict(a) #using predict method on mod , wil pass the input a to make a prediction
